#!/bin/bash

function config_pipeline() {
  echo_h2 "3. Configure Installation"

  # MAS instance ID
  if [[ ! -z "$1" ]]; then
    export MAS_INSTANCE_ID=$1
  fi
  prompt_for_input "MAS Instance ID" MAS_INSTANCE_ID

  channel_select_mas || exit 1

  echo
  echo_h2 "4. Configure Domain & Certificate Management"
  if prompt_for_confirm "Configure Custom Domain"; then
    prompt_for_input "MAS Top Level Domain" MAS_DOMAIN

    echo
    echo -e "${COLOR_YELLOW}DNS Integrations:"
    echo "  1. Cloudflare"
    echo "  2. IBM Cloud Internet Services"
    echo "  3. None (I will set up DNS myself)"
    prompt_for_input "Select DNS Provider" DNS_PROVIDER_SELECTION "1"

    case $DNS_PROVIDER_SELECTION in
      1|cloudflare)
        # User has chosen to set up DNS integration with Cloudflare
        prompt_for_input "Cloudflare e-mail" CLOUDFLARE_EMAIL
        prompt_for_input "Cloudflare API Token" CLOUDFLARE_APITOKEN
        prompt_for_input "Cloudflare Zone" CLOUDFLARE_ZONE
        prompt_for_input "Cloudflare Subdomain" CLOUDFLARE_SUBDOMAIN
        export DNS_PROVIDER=cloudflare

        echo
        echo -e "${COLOR_YELLOW}Certificate Issuer:"
        echo "  1. LetsEncrypt (Production)"
        echo "  2. LetsEncrypt (Staging)"
        echo "  3. Self-Signed"
        prompt_for_input "Select Certificate Issuer" CLUSTER_ISSUER_SELECTION "1"
        case $CLUSTER_ISSUER_SELECTION in
          1|prod)
            export MAS_CLUSTER_ISSUER="${MAS_INSTANCE_ID}-cloudflare-le-prod"
            ;;
          2|staging)
            export MAS_CLUSTER_ISSUER="${MAS_INSTANCE_ID}-cloudflare-le-stg"
            ;;
          3|self)
            export MAS_CLUSTER_ISSUER=''
            ;;
          *)
            export MAS_CLUSTER_ISSUER=CLUSTER_ISSUER_SELECTION
            ;;
        esac
        ;;
      2|cis)
        # User has chosen to set up DNS integration with Cloudflare
        prompt_for_input "CIS e-mail" CIS_EMAIL
        prompt_for_input "CIS API Key" CIS_APIKEY
        prompt_for_input "CIS CRN" CIS_CRN
        prompt_for_input "CIS Subdomain" CIS_SUBDOMAIN
        export DNS_PROVIDER=cis

        echo
        echo -e "${COLOR_YELLOW}Certificate Issuer:"
        echo "  1. LetsEncrypt (Production)"
        echo "  2. LetsEncrypt (Staging)"
        echo "  3. Self-Signed"
        prompt_for_input "Select Certificate Issuer" CLUSTER_ISSUER_SELECTION "1"
        case $CLUSTER_ISSUER_SELECTION in
          1|prod)
            export MAS_CLUSTER_ISSUER="${MAS_INSTANCE_ID}-cis-le-prod"
            ;;
          2|staging)
            export MAS_CLUSTER_ISSUER="${MAS_INSTANCE_ID}-cis-le-stg"
            ;;
          3|self)
            export MAS_CLUSTER_ISSUER=''
            ;;
          *)
            export MAS_CLUSTER_ISSUER=CLUSTER_ISSUER_SELECTION
            ;;
        esac
        ;;
      3|none)
        # User has chosen to set up DNS themselves
        export DNS_PROVIDER=''
        echo
        echo -e "${COLOR_YELLOW}Certificate Issuer:"
        echo "  1. Self-Signed"
        prompt_for_input "Select Certificate Issuer" CLUSTER_ISSUER_SELECTION "1"
        case $CLUSTER_ISSUER_SELECTION in
          1|self)
            export MAS_CLUSTER_ISSUER=''
            ;;
          *)
            export MAS_CLUSTER_ISSUER=CLUSTER_ISSUER_SELECTION
            ;;
        esac
        export MAS_CLUSTER_ISSUER=''
        ;;
      *)
        exit 1
        ;;
    esac
  else
    # User has chosen to use the default MAS domain, but can still choose a certificate issuer if they want (not supported yet)
    MAS_DOMAIN=""
    MAS_CLUSTER_ISSUER=""
    # echo
    # echo -e "${COLOR_YELLOW}Certificate Issuer:"
    # echo "  1. Self-Signed"
    # prompt_for_input "Select Certificate Issuer" CLUSTER_ISSUER_SELECTION "1"
    # case $CLUSTER_ISSUER_SELECTION in
    #   1|self)
    #     export MAS_CLUSTER_ISSUER=''
    #     ;;
    #   *)
    #     export MAS_CLUSTER_ISSUER=CLUSTER_ISSUER_SELECTION
    #     ;;
    # esac
  fi

  echo
  echo_h2 "5. Application Selection"
  # Default all applications to "do not deploy"
  export MAS_APP_SOURCE_IOT='';        export MAS_APP_CHANNEL_IOT=''
  export MAS_APP_SOURCE_MONITOR='';    export MAS_APP_CHANNEL_MONITOR=''
  export MAS_APP_SOURCE_SAFETY='';     export MAS_APP_CHANNEL_SAFETY=''
  export MAS_APP_SOURCE_MANAGE='';     export MAS_APP_CHANNEL_MANAGE=''
  export MAS_APP_SOURCE_PREDICT='';    export MAS_APP_CHANNEL_PREDICT=''
  export MAS_APP_CHANNEL_OPTIMIZER=''; export MAS_APP_SOURCE_OPTIMIZER=''; export MAS_APP_PLAN_OPTIMIZER=''

  # Applications supported in air gap and online installs
  if prompt_for_confirm "Install IoT"; then
    channel_select_iot || exit 1
  fi

  # Applications only supported in online installs
  if [[ -z "$AIRGAP_MODE" ]]; then
    # Applications that require IoT
    if [[ "$MAS_APP_CHANNEL_IOT" != '' ]]; then
      if prompt_for_confirm "Install Monitor"; then
        channel_select_monitor || exit 1
      fi
      if prompt_for_confirm "Install Safety"; then
        channel_select_safety || exit 1
      fi
    fi

    if prompt_for_confirm "Install Manage"; then
      channel_select_manage || exit 1
    fi

    # Applications that require Manage
    if [[ "$MAS_APP_CHANNEL_MANAGE" != '' ]]; then
      if prompt_for_confirm "Install Predict"; then
        channel_select_predict || exit 1
      fi
    fi

    # Optimizer can only be installed from 8.8 on
    if [[ "$MAS_CHANNEL" != '8.6.x' && "$MAS_CHANNEL" != '8.7.x' ]]; then
      if prompt_for_confirm "Install Optimizer"; then
        channel_select_optimizer || exit 1
        # Optimizer install Plan + Validation
        while : ; do
          prompt_for_input 'Optimizer Install Plan [full/limited]' MAS_APP_PLAN_OPTIMIZER "full" && export MAS_APP_PLAN_OPTIMIZER
          [[ "$MAS_APP_PLAN_OPTIMIZER" != "full" && "$MAS_APP_PLAN_OPTIMIZER" != "limited" ]] || break
        done
      fi
    fi
  fi

  # RWX: Usually this role is fulfilled by block storage classes; used by:
  # - cluster monitoring (Prometheus, Grafana, & user workload monitoring)
  # - Db2 (data, logs, and temp volumes)
  # - Kafka, MongoDb, and User Data Services
  # RWO: Usually this role is fulfilled by file storage classes; used by:
  # - cluster monitoring (alert manager)
  # - Db2 (meta and backup volumes)

  echo
  echo_h2 "6. Configure Storage Class Usage"
  echo "${TEXT_DIM}Maximo Application Suite and it's dependencies require storage classes that support ReadWriteOnce (RWO) and ReadWriteMany (RWX) access modes:"
  echo "  - ReadWriteOnce volumes can be mounted as read-write by multiple pods on a single node."
  echo "  - ReadWriteMany volumes can be mounted as read-write by multiple pods across many nodes."
  echo ""
  reset_colors
  # Auto-detect based on available storage classes
  # ----------------------------------------------
  # 1. ROKS
  oc get storageclass ibmc-file-gold &>> $LOGFILE
  if [[ $? == "0" ]]; then
    echo -e "${COLOR_GREEN}Storage provider auto-detected: IBMCloud ROKS${COLOR_RESET}"
    echo "${TEXT_DIM}  - Storage class (ReadWriteOnce): ibmc-block-gold"
    echo "${TEXT_DIM}  - Storage class (ReadWriteMany): ibmc-file-gold"
    STORAGE_CLASS_PROVIDER=ibmc
    STORAGE_CLASS_RWO=ibmc-block-gold
    STORAGE_CLASS_RWX=ibmc-file-gold
  fi

  # 2. OCS
  if [[ "$STORAGE_CLASS_RWX" == "" ]]; then
    oc get storageclass ocs-storagecluster-cephfs &>> $LOGFILE
    if [[ $? == "0" ]]; then
      PIPELINE_STORAGE_CLASS=ocs-storagecluster-cephfs
      echo -e "${COLOR_GREEN}Storage provider auto-detected: OpenShift Container Storage${COLOR_RESET}"
      echo "${TEXT_DIM}  - Storage class (ReadWriteOnce): ocs-storagecluster-ceph-rbd"
      echo "${TEXT_DIM}  - Storage class (ReadWriteMany): ocs-storagecluster-cephfs"
      STORAGE_CLASS_PROVIDER=ocs
      STORAGE_CLASS_RWO=ocs-storagecluster-ceph-rbd
      STORAGE_CLASS_RWX=ocs-storagecluster-cephfs
    fi
  fi

  # 3. Azure
  if [[ "$STORAGE_CLASS_RWX" == "" ]]; then
    oc get storageclass managed-premium &>> $LOGFILE
    if [[ $? == "0" ]]; then
      PIPELINE_STORAGE_CLASS=managed-premium
      echo -e "${COLOR_GREEN}Storage provider auto-detected: Azure Managed${COLOR_RESET}"
      echo "${TEXT_DIM}  - Storage class (ReadWriteOnce): azurefiles-premium"
      echo "${TEXT_DIM}  - Storage class (ReadWriteMany): managed-premium"
      STORAGE_CLASS_PROVIDER=azure
      STORAGE_CLASS_RWO=azurefiles-premium
      STORAGE_CLASS_RWX=managed-premium
    fi
  fi
  reset_colors

  OVERRIDE_STORAGE_CLASSES=false
  if [[ "$STORAGE_CLASS_RWX" != "" ]]; then
    echo
    prompt_for_confirm "Choose your own storage classes anyway?" OVERRIDE_STORAGE_CLASSES
  fi

  # 4. You choose then ...
  if [[ "$STORAGE_CLASS_RWX" == "" || "$OVERRIDE_STORAGE_CLASSES" == "true" ]]; then
    STORAGE_CLASS_PROVIDER=custom
    echo ""
    echo "${COLOR_YELLOW}Select the ReadWriteOnce and ReadWriteMany storage classes to use from the list below:"
    oc get storageclasses -o jsonpath='{range .items[*]}{" - "}{.metadata.name}{"\n"}{end}'
    echo ""
    prompt_for_input "ReadWriteOnce storage class" STORAGE_CLASS_RWO
    prompt_for_input "ReadWriteMany storage class" STORAGE_CLASS_RWX
  fi

  # STORAGE_CLASS_RWX
  # prometheus_alertmgr_storage_class [ibmc-file-gold-gid, ocs-storagecluster-cephfs, azurefiles-premium]
  # db2_meta_storage_class [ibmc-file-gold, ocs-storagecluster-cephfs, azurefiles-premium]
  # db2_backup_storage_class [ibmc-file-gold, ocs-storagecluster-cephfs, azurefiles-premium]

  # STORAGE_CLASS_RWO
  # prometheus_storage_class [ibmc-block-gold, ocs-storagecluster-ceph-rbd, managed-premium]
  # prometheus_userworkload_storage_class [ibmc-block-gold, ocs-storagecluster-ceph-rbd, managed-premium]
  # grafana_instance_storage_class [ibmc-block-gold, ocs-storagecluster-ceph-rbd, managed-premium]
  # db2_data_storage_class [ibmc-block-gold, ocs-storagecluster-ceph-rbd, managed-premium]
  # db2_logs_storage_class [ibmc-block-gold, ocs-storagecluster-ceph-rbd, managed-premium]
  # db2_temp_storage_class [ibmc-block-gold, ocs-storagecluster-ceph-rbd, managed-premium]
  # kafka_storage_class [ibmc-block-gold, ocs-storagecluster-ceph-rbd, managed-premium]
  # mongodb_storage_class [ibmc-block-gold, ocs-storagecluster-ceph-rbd, managed-premium]
  # uds_storage_class [ibmc-block-bronze, ocs-storagecluster-ceph-rbd, managed-premium] (doesn't really need to be bronze)

  # Unknown/TBC
  # appconnect_storage_class - doesn't support auto-select storage classes :(
  # cpd_metadata_storage_class [ibmc-block-gold, ocs-storagecluster-ceph-rbd, managed-premium]
  # cpd_primary_storage_class [ibmc-file-gold-gid, ocs-storagecluster-cephfs, azurefiles-premium]
  # cpd_service_storage_class [ibmc-file-gold-gid, ocs-storagecluster-cephfs, azurefiles-premium] / WD: [ibmc-block-gold, ocs-storagecluster-ceph-rbd, managed-premium]

  echo ""
  if [[ "$MAS_CATALOG_SOURCE" == "ibm-mas-operators" && -z "$AIRGAP_MODE" ]]; then
    # Development Mode -- offer the ability to set MAS and SLS source independently
    echo_h2 "7a. Configure Artifactory"
    prompt_for_input "Artifactory Username" ARTIFACTORY_USERNAME
    prompt_for_input "Artifactory API Key" ARTIFACTORY_APIKEY

    echo_h2 "7b. Configure IBM Container Registry"
    prompt_for_input "IBM Entitlement Key" IBM_ENTITLEMENT_KEY $IBM_ENTITLEMENT_KEY

    echo_h2 "7c. Configure IBM Container Registry (MAS)"
    prompt_for_input "IBM Container Registry (cp)" MAS_ICR_CP wiotp-docker-local.artifactory.swg-devops.com override
    prompt_for_input "IBM Container Registry (cpopen)" MAS_ICR_CPOPEN wiotp-docker-local.artifactory.swg-devops.com override
    prompt_for_input "Entitlement Username" MAS_ENTITLEMENT_USERNAME $ARTIFACTORY_USERNAME override
    prompt_for_input "Entitlement Key" MAS_ENTITLEMENT_KEY $ARTIFACTORY_APIKEY override

    echo_h2 "7d. Configure IBM Container Registry (SLS)"
    prompt_for_input "IBM Container Registry (cp)" SLS_ICR_CP wiotp-docker-local.artifactory.swg-devops.com override
    prompt_for_input "IBM Container Registry (cpopen)" SLS_ICR_CPOPEN wiotp-docker-local.artifactory.swg-devops.com override
    prompt_for_input "Entitlement Username" SLS_ENTITLEMENT_USERNAME $ARTIFACTORY_USERNAME override
    prompt_for_input "Entitlement Key" SLS_ENTITLEMENT_KEY $ARTIFACTORY_APIKEY override
  else
    # Production Mode -- everything comes from the same registry (IBM container registry)
    echo_h2 "7. Configure IBM Container Registry"
    prompt_for_input "IBM Entitlement Key" IBM_ENTITLEMENT_KEY $IBM_ENTITLEMENT_KEY

    # Use defaults
    export MAS_ICR_CP=cp.icr.io/cp
    export MAS_ICR_CPOPEN=icr.io/cpopen
    export MAS_ENTITLEMENT_USERNAME=cp
    export MAS_ENTITLEMENT_KEY=$IBM_ENTITLEMENT_KEY

    export SLS_ICR_CP=cp.icr.io/cp
    export SLS_ICR_CPOPEN=icr.io/cpopen
    export SLS_ENTITLEMENT_USERNAME=cp
    export SLS_ENTITLEMENT_KEY=$IBM_ENTITLEMENT_KEY
  fi

  if [[ -n "$AIRGAP_MODE" ]]; then
    # Override Common Services Catalog Source
    export COMMON_SERVICES_CATALOG_SOURCE=opencloud-operators
    export SLS_CATALOG_SOURCE=ibm-sls-operators
  else
    export COMMON_SERVICES_CATALOG_SOURCE=ibm-operator-catalog
    export SLS_CATALOG_SOURCE=ibm-operator-catalog
  fi

  echo
  echo_h2 "8. Configure Product License"
  prompt_for_input "License ID" SLS_LICENSE_ID
  prompt_for_input "License File" SLS_LICENSE_FILE_LOCAL
  if [[ ! -e "$SLS_LICENSE_FILE_LOCAL" ]]; then
    echo_warning "Error: File does not exist: $SLS_LICENSE_FILE_LOCAL"
    exit 1
  fi
  export SLS_LICENSE_FILE="/workspace/entitlement/$(basename $SLS_LICENSE_FILE_LOCAL)"

  echo
  echo_h2 "9. Configure UDS"
  prompt_for_input "UDS Contact Email" UDS_CONTACT_EMAIL
  prompt_for_input "UDS Contact First Name" UDS_CONTACT_FIRSTNAME
  prompt_for_input "UDS Contact Last Name" UDS_CONTACT_LASTNAME

  echo
  echo_h2 "10. Install Cloud Pak For Data"
  DEPLOY_CP4D=skip
  if [[ -z "$AIRGAP_MODE" ]]; then
    echo "Watson Studio, Watson Machine Learning, Watson Discovery, & Analytics Service will all be installed"
    prompt_for_confirm "Install Cloud Pak for Data" && DEPLOY_CP4D=run

    if [[ "$DEPLOY_CP4D" == "run" ]]; then
      case $MAS_CHANNEL in
        8.7.x)
          # Predict 8.5  was tested on 4.0.6 and 4.0.7
          # Breaking changes introduced in CP4D v4.0.8 prevent use of a newer version of CP4D
          CP4D_VERSION=4.0.7
          ;;

        8.6.x)
          # Predict 8.4 was tested on 4.0.3
          CP4D_VERSION=4.0.3
          ;;

        *)
          prompt_for_input 'Cloud Pak for Data product version' CP4D_VERSION "4.0.9"
          ;;
      esac
    fi
  else
    echo "Cloud Pak for Data install into airgap environment is not yet supported"
  fi

  echo
  echo_h2 "11. Prepare Installation"

  mkdir -p $DIR/configs
  # Replace ALL environment variables in templates
  eval "echo \"$(cat $DIR/templates/pipelinerun.yaml)\"" > $DIR/configs/pipelinerun-$MAS_INSTANCE_ID.yaml

  # Replace mas_instance_id and pipeline_storage_class in templates
  sed "s/{{mas_instance_id}}/$MAS_INSTANCE_ID/g" $DIR/templates/namespace.yaml > $DIR/configs/namespace-$MAS_INSTANCE_ID.yaml
  sed -e "s/{{mas_instance_id}}/$MAS_INSTANCE_ID/g" \
      -e "s/{{pipeline_storage_class}}/$STORAGE_CLASS_RWX/g" \
      $DIR/templates/pvc.yaml > $DIR/configs/pvc-$MAS_INSTANCE_ID.yaml
  sed "s/{{mas_instance_id}}/$MAS_INSTANCE_ID/g" $DIR/templates/rbac.yaml > $DIR/configs/rbac-$MAS_INSTANCE_ID.yaml
  sed "s/{{mas_instance_id}}/$MAS_INSTANCE_ID/g" $DIR/templates/pipeline.yaml > $DIR/configs/pipeline-$MAS_INSTANCE_ID.yaml

  if [ "$ALREADY_CONFIRMED" != "true" ]; then
    OCP_CONSOLE_ROUTE=$(oc -n openshift-console get route console -o=jsonpath='{.spec.host}')
    echo -e "Connected to OCP cluster: \n   ${COLOR_CYAN}${TEXT_UNDERLINE}https://$OCP_CONSOLE_ROUTE${TEXT_RESET}${COLOR_RESET}"
    prompt_for_confirm "Proceed with pipeline setup on this cluster" || exit 0
  fi

  echo -en "\033[s" # Save cursor position
  echo -n "Preparing namespace 'mas-$MAS_INSTANCE_ID-pipelines' ..."

  export PIPELINE_VERSION=10.4.2-pre.master
  if [ ! -e $DIR/templates/ibm-mas-devops-tasks-$PIPELINE_VERSION.yaml ]; then
    wget https://github.com/ibm-mas/ansible-devops/releases/download/$PIPELINE_VERSION/ibm-mas-devops-tasks-$PIPELINE_VERSION.yaml -O $DIR/templates/ibm-mas-devops-tasks-$PIPELINE_VERSION.yaml  &>> $LOGFILE
  fi
  # Install the MAS Devops Task definitions
  oc apply -f $DIR/configs/namespace-$MAS_INSTANCE_ID.yaml &>> $LOGFILE
  oc -n mas-$MAS_INSTANCE_ID-pipelines apply -f $DIR/templates/ibm-mas-devops-tasks-$PIPELINE_VERSION.yaml &>> $LOGFILE

  oc apply -f $DIR/configs/pvc-$MAS_INSTANCE_ID.yaml &>> $LOGFILE
  # Wait for PVC
  LOOKUP_RESULT=$(oc -n mas-$MAS_INSTANCE_ID-pipelines get pvc config-pvc -o jsonpath='{.status.phase}')
  while [ "$LOOKUP_RESULT" != "Bound" ]; do
    echo "Waiting 5s for PVC to be bound before checking again ..."  &>> $LOGFILE
    sleep 5
    LOOKUP_RESULT=$(oc -n mas-$MAS_INSTANCE_ID-pipelines get pvc config-pvc -o jsonpath='{.status.phase}')
  done

  oc apply -f $DIR/configs/rbac-$MAS_INSTANCE_ID.yaml &>> $LOGFILE
  oc apply -f $DIR/configs/pipeline-$MAS_INSTANCE_ID.yaml &>> $LOGFILE

  # Clean up existing secrets
  oc -n mas-$MAS_INSTANCE_ID-pipelines delete secret pipeline-additional-configs --ignore-not-found=true &>> $LOGFILE
  oc -n mas-$MAS_INSTANCE_ID-pipelines delete secret pipeline-sls-entitlement --ignore-not-found=true &>> $LOGFILE

  # Create new secrets
  # pipeline-additional-configs must exist (otherwise the suite-install step will hang),
  # but can be empty if no additional configs are required
  # TODO: Support passing in files to this secret
  oc -n mas-$MAS_INSTANCE_ID-pipelines create secret generic pipeline-additional-configs &>> $LOGFILE
  oc -n mas-$MAS_INSTANCE_ID-pipelines create secret generic pipeline-sls-entitlement --from-file=$SLS_LICENSE_FILE_LOCAL &>> $LOGFILE

    echo -en "\033[1K" # Clear current line
    echo -en "\033[u" # Restore cursor position
    echo -e "${COLOR_GREEN}Namespace 'mas-$MAS_INSTANCE_ID-pipelines' is ready${COLOR_RESET}"

}
